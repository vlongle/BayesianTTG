{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first reproduce their _Coalitional Bargaining with Agent Type Uncertainty_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simplifications: \n",
    "- only division rule: proportional to weights (perceived by the proposers)\n",
    "- finite types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the fixed-type is common observable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from bisect import bisect_left, bisect_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, active_agents, t):\n",
    "        self.active_agents = active_agents\n",
    "        self.t = t\n",
    "    \n",
    "    def expand_states(self, game):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_vector(agent_type, T):\n",
    "    v = np.zeros(T)\n",
    "    v[agent_type] = 1\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, name, agent_type):\n",
    "        self.agent_type = agent_type\n",
    "        self.policy = []\n",
    "        self.name = str(name) + ':' + str(self.agent_type)\n",
    "        self.belief = {} # key = name, value = prob. vector over finite type\n",
    "        \n",
    "    def init_belief(self, game):\n",
    "        '''\n",
    "        Initially, uniform prior over other agents\n",
    "        '''\n",
    "        self.belief[self.name] = one_hot_vector(self.agent_type, game.T)\n",
    "        for player in game.agents:\n",
    "            if player.name == self.name:\n",
    "                continue\n",
    "            self.belief[player.name] = np.array([1/game.T for _ in range(game.T)])\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.name\n",
    "    \n",
    "    def draw_types(self):\n",
    "        '''\n",
    "        draw the types of other agents based on current belief\n",
    "        '''\n",
    "        self.belief_types = {} # key = name, value = drawn type\n",
    "        for player_name, belief_prob in self.belief.items():\n",
    "            self.belief_types[player_name] = np.random.choice(range(len(belief_prob)),p=belief_prob)\n",
    "    \n",
    "    def proposer_eval(self, state):\n",
    "        pass\n",
    "    \n",
    "    def responder_eval(self, state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, T, N, tasks, horizon):\n",
    "        self.T = T\n",
    "        self.N = N\n",
    "        self.tasks = tasks\n",
    "        self.horizon = horizon\n",
    "        self.nodes = defaultdict(set)\n",
    "        self.agents = []\n",
    "        self.init_game()\n",
    "        \n",
    "    def init_game(self):\n",
    "        # chr(65)='A', chr(66)='B' and so on\n",
    "        self.agents = [Agent(chr(i+65), np.random.randint(0, self.T)) \\\n",
    "                          for i in range(self.N)]\n",
    "        self.init_beliefs()\n",
    "    \n",
    "    def init_beliefs(self):\n",
    "        for player in self.agents:\n",
    "            player.init_belief(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task:\n",
    "    def __init__(self, threshold, reward):\n",
    "        self.threshold = threshold\n",
    "        self.reward = reward \n",
    "        self.name = 'Task(threshold={},reward={})'.format(self.threshold, self.reward)\n",
    "    def __repr__(self):\n",
    "        return self.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 3 # no. of types\n",
    "N = 5 # no. of players\n",
    "horizon = 2 # how many proposal rounds before a game terminate\n",
    "tasks = [Task(threshold=1, reward=1), \n",
    "         Task(threshold=4, reward=2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Game(T, N, tasks, horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[A:0, B:0, C:1, D:0, E:0]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Task(threshold=1,reward=1), Task(threshold=4,reward=2)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A:0': array([1., 0., 0.]),\n",
       " 'B:0': array([0.33333333, 0.33333333, 0.33333333]),\n",
       " 'C:1': array([0.33333333, 0.33333333, 0.33333333]),\n",
       " 'D:0': array([0.33333333, 0.33333333, 0.33333333]),\n",
       " 'E:0': array([0.33333333, 0.33333333, 0.33333333])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.agents[0].belief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A:0': 0, 'B:0': 1, 'C:1': 0, 'D:0': 1, 'E:0': 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.agents[0].draw_types()\n",
    "g.agents[0].belief_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly choose a proposer\n",
    "#proposer = np.random.choice(g.active_agents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the responder node, we can have many different agent decision assumptions.\n",
    "-  Other agent = decider assumption: eqn 2 of Chalkiadakis.\n",
    "- You're = decider.\n",
    "- Softmax/proportional prob. instead of argmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([1,2,3], p=[0.5,0.5,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Continuation payoff calculation in Chalkiadakis is weird__\n",
    "\n",
    "Let's us do our thing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a type vector of common knowledge, we can evaluate the whole game tree!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$V(G) = \\frac{1}{n} \\sum_{i=1}^n V(G_i)$\n",
    "where $V(G_i)$ is the value of the proposal node where $i$ proposes. \n",
    "\n",
    "$G_i$ would look at all potential coalition $C$ containing $i$. Each coalition either leads to acceptance or rejection based on the type vector $t$ (not probabilistic but deterministic so here!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A responder $j$ under this common knowledge would accept if their payoff (they know the common knowledge) is larger than the continuation payoff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Our approach__\n",
    "\n",
    "I mean we ultimately assume what we assume that i knows about j.\n",
    "\n",
    "\n",
    "Ok. Let us assume that agent i is quite egotistical and assume that his belief is so \"obvious\" that it ought to be a common knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_coalition(C, tasks):\n",
    "    '''\n",
    "    C is a list of agent weight!\n",
    "    '''\n",
    "    W = sum(C)\n",
    "    thresholds = sorted([t.threshold for t in tasks]) \n",
    "    print(tasks)\n",
    "    insertion_pt = bisect_right(thresholds, W)\n",
    "    if insertion_pt == 0:\n",
    "        return None\n",
    "    return tasks[insertion_pt-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Task(threshold=1,reward=1), Task(threshold=4,reward=2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Task(threshold=1,reward=1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_coalition([0.5, 1], tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2/(2 + 1 + 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.475"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1/(2+1+1)) * 5.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = set(['A', 'B'])\n",
    "s2 = set(['B'])\n",
    "s1.difference(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A', 'B'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three assumptions.\n",
    "\n",
    "1. Common knowledge in mental simulation!\n",
    "2. Greedily optimize in mental simulation\n",
    "3. One wrong at the time in the belief update!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Bayesian update is obviously quite off due to the observer's lack of access to the knowledge of other agents.\n",
    "\n",
    "The observer reasons about the likelihood of a responder using the observer's own belief. However, in reality, the responder uses their belief!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the Paper uses the latter case. No wonder there would be some sort of convergence in beliefs to the correct one. The observer basically gains indirect access to the responder's type (I think ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "product()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": false,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": false,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
